{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Q Learner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhuvanabasapur/CMPE-297/blob/main/Copy_of_Q_Learner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wN8nyTylzCCq",
        "outputId": "e3e082fb-46ba-4cc3-aca2-f051ebebcfec"
      },
      "source": [
        "%%bash\n",
        "\n",
        "# install required system dependencies\n",
        "apt-get install -y xvfb x11-utils\n",
        "\n",
        "# install required python dependencies (might need to install additional gym extras depending)\n",
        "pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  libxxf86dga1\n",
            "Suggested packages:\n",
            "  mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  libxxf86dga1 x11-utils xvfb\n",
            "0 upgraded, 3 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 994 kB of archives.\n",
            "After this operation, 2,981 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.9 [784 kB]\n",
            "Fetched 994 kB in 1s (1,091 kB/s)\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 155047 files and directories currently installed.)\r\n",
            "Preparing to unpack .../libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\r\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\r\n",
            "Selecting previously unselected package x11-utils.\r\n",
            "Preparing to unpack .../x11-utils_7.7+3build1_amd64.deb ...\r\n",
            "Unpacking x11-utils (7.7+3build1) ...\r\n",
            "Selecting previously unselected package xvfb.\r\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\r\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\r\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\r\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\r\n",
            "Setting up x11-utils (7.7+3build1) ...\r\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\r\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\r\n",
            "\r\n",
            "Requirement already satisfied: gym[box2d]==0.17.* in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Collecting pyvirtualdisplay==0.2.*\n",
            "  Downloading PyVirtualDisplay-0.2.5-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: PyOpenGL==3.1.* in /usr/local/lib/python3.7/dist-packages (3.1.5)\n",
            "Collecting PyOpenGL-accelerate==3.1.*\n",
            "  Downloading PyOpenGL-accelerate-3.1.5.tar.gz (538 kB)\n",
            "Collecting EasyProcess\n",
            "  Downloading EasyProcess-0.3-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.5.0)\n",
            "Collecting box2d-py~=2.3.5\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17.*) (0.16.0)\n",
            "Building wheels for collected packages: PyOpenGL-accelerate\n",
            "  Building wheel for PyOpenGL-accelerate (setup.py): started\n",
            "  Building wheel for PyOpenGL-accelerate (setup.py): finished with status 'done'\n",
            "  Created wheel for PyOpenGL-accelerate: filename=PyOpenGL_accelerate-3.1.5-cp37-cp37m-linux_x86_64.whl size=1599533 sha256=d763df88f3ea378ba549028f00c7f9c534beea7ea4fc81e4f42c17e417a23a7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/f5/6f/169afb3f2d476c5e807f8515b3c9bc9b819c3962316aa804eb\n",
            "Successfully built PyOpenGL-accelerate\n",
            "Installing collected packages: EasyProcess, box2d-py, pyvirtualdisplay, PyOpenGL-accelerate\n",
            "Successfully installed EasyProcess-0.3 PyOpenGL-accelerate-3.1.5 box2d-py-2.3.8 pyvirtualdisplay-0.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlRSAWorzKZk",
        "outputId": "387e128e-d98d-42b9-af96-2885c9c9992e"
      },
      "source": [
        "#import\n",
        "import pyvirtualdisplay\n",
        "_display = pyvirtualdisplay.Display(visible=0,  # remember to use visible=0 and not False\n",
        "                                    size=(1400, 900))\n",
        "_ = _display.start()\n",
        "\n",
        "#check\n",
        "!echo $DISPLAY"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":1001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zN-loY7OYNBF",
        "outputId": "8346068c-814b-4b43-bfa6-0fd458b7bc7b"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 22.8 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 30 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 71 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 124 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0KzizRJzp42"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "from IPython import display\n",
        "from tensorboardX import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrEWBs0tXRWF"
      },
      "source": [
        "ENV_NAME = \"FrozenLake-v0\"\n",
        "GAMMA = 0.9\n",
        "ALPHA = 0.2\n",
        "TEST_EPISODES = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_PH05LQXsRB"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV_NAME)\n",
        "        self.state = self.env.reset()\n",
        "        self.values = collections.defaultdict(float)\n",
        "\n",
        "    def sample_env(self):\n",
        "        action = self.env.action_space.sample()\n",
        "        old_state = self.state\n",
        "        new_state, reward, is_done, _ = self.env.step(action)\n",
        "        self.state = self.env.reset() if is_done else new_state\n",
        "        return (old_state, action, reward, new_state)\n",
        "\n",
        "    def best_value_and_action(self, state):\n",
        "        best_value, best_action = None, None\n",
        "        for action in range(self.env.action_space.n):\n",
        "            action_value = self.values[(state, action)]\n",
        "            if best_value is None or best_value < action_value:\n",
        "                best_value = action_value\n",
        "                best_action = action\n",
        "        return best_value, best_action\n",
        "\n",
        "    def value_update(self, s, a, r, next_s):\n",
        "        best_v, _ = self.best_value_and_action(next_s)\n",
        "        new_val = r + GAMMA * best_v\n",
        "        old_val = self.values[(s, a)]\n",
        "        self.values[(s, a)] = old_val * (1-ALPHA) + new_val * ALPHA\n",
        "\n",
        "    def play_episode(self, env):\n",
        "        total_reward = 0.0\n",
        "        state = env.reset()\n",
        "        while True:\n",
        "            _, action = self.best_value_and_action(state)\n",
        "            new_state, reward, is_done, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "            if is_done:\n",
        "                break\n",
        "            state = new_state\n",
        "        return total_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvCma4jeZx8K"
      },
      "source": [
        "#Case 1:\n",
        "GAMMA = 0.9\n",
        "\n",
        "ALPHA = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNLnCc9NX5D-",
        "outputId": "2885baf8-4f3b-4afc-850a-32c921b4b1bc"
      },
      "source": [
        "    test_env = gym.make(ENV_NAME)\n",
        "    agent = Agent()\n",
        "    writer = SummaryWriter(comment=\"-q-learning\")\n",
        "\n",
        "    iter_no = 0\n",
        "    best_reward = 0.0\n",
        "    while True:\n",
        "        iter_no += 1\n",
        "        s, a, r, next_s = agent.sample_env()\n",
        "        agent.value_update(s, a, r, next_s)\n",
        "\n",
        "        reward = 0.0\n",
        "        for _ in range(TEST_EPISODES):\n",
        "            reward += agent.play_episode(test_env)\n",
        "        reward /= TEST_EPISODES\n",
        "        writer.add_scalar(\"reward\", reward, iter_no)\n",
        "        if reward > best_reward:\n",
        "            print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
        "            best_reward = reward\n",
        "        if reward > 0.80:\n",
        "            print(\"Solved in %d iterations!\" % iter_no)\n",
        "            break\n",
        "    writer.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best reward updated 0.000 -> 0.050\n",
            "Best reward updated 0.050 -> 0.100\n",
            "Best reward updated 0.100 -> 0.150\n",
            "Best reward updated 0.150 -> 0.200\n",
            "Best reward updated 0.200 -> 0.250\n",
            "Best reward updated 0.250 -> 0.300\n",
            "Best reward updated 0.300 -> 0.400\n",
            "Best reward updated 0.400 -> 0.550\n",
            "Best reward updated 0.550 -> 0.600\n",
            "Best reward updated 0.600 -> 0.650\n",
            "Best reward updated 0.650 -> 0.700\n",
            "Best reward updated 0.700 -> 0.750\n",
            "Best reward updated 0.750 -> 0.850\n",
            "Solved in 8320 iterations!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVlqjNeFZ7zr"
      },
      "source": [
        "##Case 2:\n",
        "GAMMA = 0.95\n",
        "\n",
        "ALPHA = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0S57J4daEqx"
      },
      "source": [
        "GAMMA = 0.95\n",
        "ALPHA = 0.2"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96R-3P9IaH8H",
        "outputId": "c0316c3c-b169-40ba-ad1c-c8b2cf3cdddb"
      },
      "source": [
        "    test_env = gym.make(ENV_NAME)\n",
        "    agent = Agent()\n",
        "    writer = SummaryWriter(comment=\"-q-learning\")\n",
        "\n",
        "    iter_no = 0\n",
        "    best_reward = 0.0\n",
        "    while True:\n",
        "        iter_no += 1\n",
        "        s, a, r, next_s = agent.sample_env()\n",
        "        agent.value_update(s, a, r, next_s)\n",
        "\n",
        "        reward = 0.0\n",
        "        for _ in range(TEST_EPISODES):\n",
        "            reward += agent.play_episode(test_env)\n",
        "        reward /= TEST_EPISODES\n",
        "        writer.add_scalar(\"reward\", reward, iter_no)\n",
        "        if reward > best_reward:\n",
        "            print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
        "            best_reward = reward\n",
        "        if reward > 0.80:\n",
        "            print(\"Solved in %d iterations!\" % iter_no)\n",
        "            break\n",
        "    writer.close()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best reward updated 0.000 -> 0.150\n",
            "Best reward updated 0.150 -> 0.300\n",
            "Best reward updated 0.300 -> 0.350\n",
            "Best reward updated 0.350 -> 0.450\n",
            "Best reward updated 0.450 -> 0.500\n",
            "Best reward updated 0.500 -> 0.550\n",
            "Best reward updated 0.550 -> 0.750\n",
            "Best reward updated 0.750 -> 0.800\n",
            "Best reward updated 0.800 -> 0.850\n",
            "Solved in 5924 iterations!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IevYZj84a4Fm"
      },
      "source": [
        "##Case 3: \n",
        "GAMMA = 0.9\n",
        "ALPHA = 0.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXpaC3lra3ao"
      },
      "source": [
        "GAMMA = 0.9\n",
        "ALPHA = 0.3"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4UjY3J0bBPr",
        "outputId": "b92d4089-95c5-4a1a-c46a-8134926f210b"
      },
      "source": [
        "    test_env = gym.make(ENV_NAME)\n",
        "    agent = Agent()\n",
        "    writer = SummaryWriter(comment=\"-q-learning\")\n",
        "\n",
        "    iter_no = 0\n",
        "    best_reward = 0.0\n",
        "    while True:\n",
        "        iter_no += 1\n",
        "        s, a, r, next_s = agent.sample_env()\n",
        "        agent.value_update(s, a, r, next_s)\n",
        "\n",
        "        reward = 0.0\n",
        "        for _ in range(TEST_EPISODES):\n",
        "            reward += agent.play_episode(test_env)\n",
        "        reward /= TEST_EPISODES\n",
        "        writer.add_scalar(\"reward\", reward, iter_no)\n",
        "        if reward > best_reward:\n",
        "            print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
        "            best_reward = reward\n",
        "        if reward > 0.80:\n",
        "            print(\"Solved in %d iterations!\" % iter_no)\n",
        "            break\n",
        "    writer.close()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best reward updated 0.000 -> 0.050\n",
            "Best reward updated 0.050 -> 0.200\n",
            "Best reward updated 0.200 -> 0.250\n",
            "Best reward updated 0.250 -> 0.300\n",
            "Best reward updated 0.300 -> 0.400\n",
            "Best reward updated 0.400 -> 0.500\n",
            "Best reward updated 0.500 -> 0.600\n",
            "Best reward updated 0.600 -> 0.700\n",
            "Best reward updated 0.700 -> 0.800\n",
            "Best reward updated 0.800 -> 0.850\n",
            "Solved in 3916 iterations!\n"
          ]
        }
      ]
    }
  ]
}